{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-02-14T09:25:41.236728Z","iopub.execute_input":"2026-02-14T09:25:41.237522Z","iopub.status.idle":"2026-02-14T09:25:41.241822Z","shell.execute_reply.started":"2026-02-14T09:25:41.237491Z","shell.execute_reply":"2026-02-14T09:25:41.241217Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"!pip install bitsandbytes accelerate transformers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-14T09:25:24.771007Z","iopub.execute_input":"2026-02-14T09:25:24.771215Z","iopub.status.idle":"2026-02-14T09:25:31.127142Z","shell.execute_reply.started":"2026-02-14T09:25:24.771193Z","shell.execute_reply":"2026-02-14T09:25:31.126273Z"}},"outputs":[{"name":"stdout","text":"Collecting bitsandbytes\n  Downloading bitsandbytes-0.49.1-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\nRequirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.11.0)\nRequirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\nRequirement already satisfied: torch<3,>=2.3 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.8.0+cu126)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.0.2)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (26.0rc2)\nRequirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from accelerate) (6.0.3)\nRequirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (0.36.0)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from accelerate) (0.6.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.3)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\nRequirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.5)\nRequirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (2025.10.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (4.15.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (1.2.1rc0)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (75.2.0)\nRequirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.13.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.80)\nRequirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (9.10.2.21)\nRequirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.4.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.3.0.4)\nRequirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (10.3.7.77)\nRequirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.7.1.2)\nRequirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.5.4.2)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (0.7.1)\nRequirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2.27.3)\nRequirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.85)\nRequirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.11.1.6)\nRequirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.4.0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.6.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2026.1.4)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<3,>=2.3->bitsandbytes) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<3,>=2.3->bitsandbytes) (3.0.3)\nDownloading bitsandbytes-0.49.1-py3-none-manylinux_2_24_x86_64.whl (59.1 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m33.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: bitsandbytes\nSuccessfully installed bitsandbytes-0.49.1\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import sys\nimport os\nfrom typing import List, Tuple, Optional, Dict\n\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSequenceClassification\nimport torch\nfrom transformers import pipeline\nimport torch.nn as nn\nfrom sentence_transformers import SentenceTransformer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-14T08:58:38.310052Z","iopub.execute_input":"2026-02-14T08:58:38.310693Z","iopub.status.idle":"2026-02-14T08:58:38.314548Z","shell.execute_reply.started":"2026-02-14T08:58:38.310660Z","shell.execute_reply":"2026-02-14T08:58:38.313883Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\nimport gc # Added for memory management\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n\nclass ModelProvider:\n    def __init__(self, model_name: str):\n        self.model_name = model_name\n        self.tokenizer = None\n        self.model = None\n        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n    def load(self):\n        print(f\"ðŸ“¥ Loading tokenizer: {self.model_name}\")\n        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n        \n        # Ensure padding token exists for batching/safety\n        if self.tokenizer.pad_token is None:\n            self.tokenizer.pad_token = self.tokenizer.eos_token\n\n        print(\"âš¡ Enabling 4-bit quantization (NF4)...\")\n        bnb_config = BitsAndBytesConfig(\n            load_in_4bit=True,\n            bnb_4bit_quant_type=\"nf4\",\n            bnb_4bit_compute_dtype=torch.float16,\n            bnb_4bit_use_double_quant=True,\n        )\n\n        print(\"ðŸ§  Loading model to GPU...\")\n        self.model = AutoModelForCausalLM.from_pretrained(\n            self.model_name,\n            quantization_config=bnb_config,\n            device_map=\"auto\", \n            torch_dtype=torch.float16,\n            low_cpu_mem_usage=True\n        )\n\n        print(f\"âœ… Model loaded on: {self.model.device}\")\n        return self\n\n    def generate_with_logprobs(self, prompt: str, max_new_tokens: int = 150, temperature: float = 0.7):\n        # 1. Clear cache before generation to prevent OOM\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n            gc.collect()\n\n        inputs = self.tokenizer(prompt, return_tensors=\"pt\")\n        input_ids = inputs[\"input_ids\"].to(self.model.device)\n        input_len = input_ids.shape[1]\n\n        outputs = self.model.generate(\n            input_ids=input_ids,\n            max_new_tokens=max_new_tokens,\n            temperature=temperature,\n            do_sample=True,\n            return_dict_in_generate=True,\n            output_scores=True,\n            pad_token_id=self.tokenizer.pad_token_id\n        )\n\n        sequences = outputs.sequences[0]\n        scores = outputs.scores\n        \n        # 2. Decode text safely\n        full_text = self.tokenizer.decode(sequences, skip_special_tokens=True)\n        # Using a more robust split for the prompt echo\n        decoded_prompt = self.tokenizer.decode(input_ids[0], skip_special_tokens=True)\n        generated_text = full_text.replace(decoded_prompt, \"\").strip()\n\n        # 3. Log Probability Computation with safety zip\n        logprobs = []\n        gen_token_ids = sequences[input_len:]\n\n        # Use zip to prevent IndexError if EOS token stops generation early\n        for step_logits, token_id in zip(scores, gen_token_ids):\n            step_logprobs = F.log_softmax(step_logits[0], dim=-1)\n            logprobs.append(step_logprobs[token_id].item())\n\n        avg_logprob = sum(logprobs) / len(logprobs) if logprobs else 0.0\n\n        return {\n            \"text\": generated_text,\n            \"logprobs\": logprobs,\n            \"avg_logprob\": avg_logprob\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-14T09:37:23.027522Z","iopub.execute_input":"2026-02-14T09:37:23.028236Z","iopub.status.idle":"2026-02-14T09:37:23.038720Z","shell.execute_reply.started":"2026-02-14T09:37:23.028202Z","shell.execute_reply":"2026-02-14T09:37:23.037763Z"}},"outputs":[],"execution_count":48},{"cell_type":"code","source":"\n\nfrom langchain_core.prompts import ChatPromptTemplate\n\nclass PromptLibrary:\n    def __init__(self):\n        self.supporting_generator_prompt = ChatPromptTemplate.from_messages([\n            (\"system\", \"You are a prolific debater, your task is to generate arguments on the given topic and position.\"),\n            (\"human\", \"{topic}...\")\n        ])\n        self.opposing_generator_prompt = ChatPromptTemplate.from_messages([\n            (\"system\", \"You are a prolific debater, your task is to generate arguments against the given topic and position.\"),\n            (\"human\", \"{topic}...\")\n        ])\n        self.nli_critic_prompt = ChatPromptTemplate.from_messages([\n            (\"system\", \"You are a critic of arguments. Evaluate the relevance of the argument to the premise.\"),\n            (\"human\", \"Premise: {premise}\\nArgument: {argument}\")\n        ])\n        self.supporting_merger_prompt = ChatPromptTemplate.from_messages([\n            (\"system\", \"You are an expert debater. You have been given supporting arguments  and counter arguments to oppose the opposition on a topic. Your task is to merge them into a single  argument for the topic.\"),\n            (\"human\", \"Supporting Argument: {supporting}\\nOpposing Argument: {opposing}\")\n        ])\n        self.opposing_merger_prompt = ChatPromptTemplate.from_messages([\n            (\"system\", \"You are an expert debater. You have been given supporting arguments  and counter arguments to oppose the opposition on a topic. Your task is to merge them into a single  argument against the topic.\"),\n            (\"human\", \"Supporting Argument: {supporting}\\nOpposing Argument: {opposing}\")\n        ])\n        # ... all other prompts","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-14T09:30:26.309641Z","iopub.execute_input":"2026-02-14T09:30:26.309948Z","iopub.status.idle":"2026-02-14T09:30:26.315530Z","shell.execute_reply.started":"2026-02-14T09:30:26.309922Z","shell.execute_reply":"2026-02-14T09:30:26.314662Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"def nli_relevance_check(nli_pipeline,premise: str, hypothesis: str) -> bool:\n    \"\"\"\n    Perform a simple NLI relevance check between the premise and hypothesis.\n    This is a placeholder function and should be replaced with an actual NLI model for production use.\n    \"\"\"\n    \n    \n    result = nli_pipeline(f\"{premise} [SEP] {hypothesis}\")   \n    return {\n        \"label\": result[0]['label'],\n        \"score\": result[0]['score']\n    }\ndef similarity_check(model, text1: str, text2: str) -> float:\n    # 1. Compute embeddings for both sentences\n    # SentenceTransformer.encode returns a numpy array or torch tensor\n    embeddings = model.encode([text1, text2], convert_to_tensor=True)\n    \n    # 2. Compute Cosine Similarity\n    # util.cos_sim returns a matrix; we take [0][1] for the score between text1 and text2\n    cosine_score = util.cos_sim(embeddings[0], embeddings[1])\n    \n    # 3. Scale from [-1, 1] to [0, 1]\n    scaled_score = (cosine_score.item() + 1) / 2\n    \n    return scaled_score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-14T09:27:14.454618Z","iopub.execute_input":"2026-02-14T09:27:14.455183Z","iopub.status.idle":"2026-02-14T09:27:14.459695Z","shell.execute_reply.started":"2026-02-14T09:27:14.455155Z","shell.execute_reply":"2026-02-14T09:27:14.458979Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"from datasets import load_dataset\n\n# Login using e.g. `huggingface-cli login` to access this dataset\nds = load_dataset(\"ibm-research/argument_quality_ranking_30k\", \"argument_quality_ranking\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-14T09:27:19.490304Z","iopub.execute_input":"2026-02-14T09:27:19.490633Z","iopub.status.idle":"2026-02-14T09:27:19.925408Z","shell.execute_reply.started":"2026-02-14T09:27:19.490600Z","shell.execute_reply":"2026-02-14T09:27:19.924888Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"\n\ndef supporting_argument_node(state, model_provider: ModelProvider):\n    topic = state[\"topic\"]\n\n    prompt = PromptLibrary().supporting_generator_prompt.format(\n        topic=topic\n    )\n\n    result = model_provider.generate_with_logprobs(prompt)\n\n    return {\n        \"argument\": result[\"text\"],\n        \"logprobs\": result[\"logprobs\"],\n        \"avg_logprob\": result[\"avg_logprob\"]\n    }\n\n\ndef opposing_argument_node(state, model_provider: ModelProvider):\n    topic = state[\"topic\"]\n\n    prompt = PromptLibrary().opposing_generator_prompt(\n        topic=topic\n    )\n\n    result = model_provider.generate_with_logprobs(prompt)\n\n    return {\n        \"argument\": result[\"text\"],\n        \"logprobs\": result[\"logprobs\"],\n        \"avg_logprob\": result[\"avg_logprob\"]\n    }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-14T09:30:33.429548Z","iopub.execute_input":"2026-02-14T09:30:33.429873Z","iopub.status.idle":"2026-02-14T09:30:33.436180Z","shell.execute_reply.started":"2026-02-14T09:30:33.429845Z","shell.execute_reply":"2026-02-14T09:30:33.435472Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"import torch\nfrom transformers import pipeline\nfrom sentence_transformers import SentenceTransformer\n\n\nclass FeatureExtractor:\n\n    def __init__(self):\n\n        # =====================================================\n        # Device Setup\n        # =====================================================\n        self.device_id = 0 if torch.cuda.is_available() else -1\n        self.device_str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n        print(f\"ðŸš€ FeatureExtractor using device: {self.device_str}\")\n\n        # =====================================================\n        # NLI Pipeline\n        # =====================================================\n        self.nli_pipeline = pipeline(\n            \"text-classification\",\n            model=\"roberta-large-mnli\",\n            device=self.device_id\n        )\n\n        # =====================================================\n        # Similarity Model\n        # =====================================================\n        self.similarity_model = SentenceTransformer(\n            \"all-MiniLM-L6-v2\",\n            device=self.device_str\n        )\n\n        # =====================================================\n        # Model Provider (LLM)\n        # =====================================================\n        self.model_provider = ModelProvider(\n            model_name=\"mistralai/Mistral-7B-Instruct-v0.1\",\n            \n        )\n\n        print(\"ðŸ“¥ Loading Model Weights & Tokenizer...\")\n        self.model_provider.load()\n\n    # =========================================================\n    # Feature Extraction\n    # =========================================================\n    def extract_features(self, topic: str):\n\n        # --- Generate statements ---\n        supporting_result = supporting_argument_node(\n            {\"topic\": topic},\n            self.model_provider\n        )\n\n        opposing_result = opposing_argument_node(\n            {\"topic\": topic},\n            self.model_provider\n        )\n\n        avg_logprob = supporting_result[\"avg_logprob\"]\n        argument = supporting_result[\"argument\"]\n        opposing_argument = opposing_result[\"argument\"]\n\n        # =====================================================\n        # 1ï¸âƒ£ Relevance (Topic â†’ Support)\n        # =====================================================\n        relevance_result = nli_relevance_check(\n            self.nli_pipeline,\n            topic,\n            argument\n        )\n\n        if relevance_result[\"label\"] == \"ENTAILMENT\":\n            relevance_score_support = relevance_result[\"score\"]\n        elif relevance_result[\"label\"] == \"CONTRADICTION\":\n            relevance_score_support = 0.0\n        else:  # NEUTRAL\n            relevance_score_support = 1 - relevance_result[\"score\"]\n\n        # =====================================================\n        # 2ï¸âƒ£ Relevance (Support â†” Opposing)\n        # =====================================================\n        relevance_result2 = nli_relevance_check(\n            self.nli_pipeline,\n            argument,\n            opposing_argument\n        )\n\n        if relevance_result2[\"label\"] == \"ENTAILMENT\":\n            relevance_score_contra = 1 - relevance_result2[\"score\"]\n        elif relevance_result2[\"label\"] == \"NEUTRAL\":\n            relevance_score_contra = 0.0\n        else:  # CONTRADICTION\n            relevance_score_contra = relevance_result2[\"score\"]\n\n        # =====================================================\n        # 3ï¸âƒ£ Semantic Similarity\n        # =====================================================\n        similarity_score = similarity_check(\n            self.similarity_model,\n            topic,\n            argument\n        )\n\n        return {\n            \"avg_logprob\": avg_logprob,\n            \"relevance_score_support\": relevance_score_support,\n            \"relevance_score_contra\": relevance_score_contra,\n            \"similarity_score\": similarity_score\n        }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-14T09:38:49.087226Z","iopub.execute_input":"2026-02-14T09:38:49.088025Z","iopub.status.idle":"2026-02-14T09:38:49.102538Z","shell.execute_reply.started":"2026-02-14T09:38:49.087984Z","shell.execute_reply":"2026-02-14T09:38:49.101635Z"}},"outputs":[],"execution_count":50},{"cell_type":"code","source":"from typing import List\ndef prepare_data(data) -> List[:]:\n    feature_extractor = FeatureExtractor()\n    training_data = []\n    for d in data:\n        topic= d[\"topic\"]\n        truth_score=d[\"WA\"]*d[\"stance_WA\"]\n        features=feature_extractor.extract_features(topic)\n        training_data.append([topic,features,truth_score ])\n    return training_data\n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-14T09:38:50.383549Z","iopub.execute_input":"2026-02-14T09:38:50.384369Z","iopub.status.idle":"2026-02-14T09:38:50.388881Z","shell.execute_reply.started":"2026-02-14T09:38:50.384330Z","shell.execute_reply":"2026-02-14T09:38:50.388033Z"}},"outputs":[],"execution_count":51},{"cell_type":"code","source":"training_data=prepare_data(ds[\"train\"])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-14T09:38:55.497760Z","iopub.execute_input":"2026-02-14T09:38:55.498432Z"}},"outputs":[{"name":"stdout","text":"ðŸš€ FeatureExtractor using device: cuda\n","output_type":"stream"},{"name":"stderr","text":"Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nDevice set to use cuda:0\n","output_type":"stream"},{"name":"stdout","text":"ðŸ“¥ Loading Model Weights & Tokenizer...\nðŸ“¥ Loading tokenizer: mistralai/Mistral-7B-Instruct-v0.1\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"32389b2d86fa46f7a963f1daacc59844"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f6e380c14aed4ad6bb0a088e54ae5958"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e32b9e887b4431d89be920e756914fc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d739f7035db147299af6ddd66146c193"}},"metadata":{}},{"name":"stdout","text":"âš¡ Enabling 4-bit quantization (NF4)...\nðŸ§  Loading model to GPU...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fbae408d6a3642f393bb64ed9ac48689"}},"metadata":{}},{"name":"stderr","text":"`torch_dtype` is deprecated! Use `dtype` instead!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8eca220b2a3a4522bd90ea7cdeb256fa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3a487c3b86b14e67918d3ee4a50c6f78"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/9.94G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6a7345d993cb448f83e295677227bda5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"48eb3ee2c9734512af27a623352829cc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ff8056d13e86426ab6d3cf2094cd8992"}},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}